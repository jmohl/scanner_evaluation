{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan Results Analysis\n",
    "\n",
    "Compares scanner outputs across all runs in `core_bench/scan-results/`, with a focus on **validation agreement** — how well the scanner's numeric predictions match the hand-labeled targets stored in each `_scan.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from inspect_scout import scan_results_df\n",
    "\n",
    "SCAN_ROOT = Path(\"scan-results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load all scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_dirs = sorted(SCAN_ROOT.glob(\"scan_id=*\"))\n",
    "\n",
    "scan_meta = []\n",
    "scan_dfs  = {}   # {scan_id: {scanner_name: df}}\n",
    "\n",
    "for d in scan_dirs:\n",
    "    scan_id = d.name.split(\"=\", 1)[1]\n",
    "    meta = json.loads((d / \"_scan.json\").read_text())\n",
    "    scan_meta.append({\n",
    "        \"scan_id\":    scan_id,\n",
    "        \"timestamp\":  meta[\"timestamp\"],\n",
    "        \"model\":      meta[\"model\"][\"model\"],\n",
    "        \"scanners\":   list(meta[\"scanners\"].keys()),\n",
    "        \"has_validation\": list(meta.get(\"validation\", {}).keys()),\n",
    "    })\n",
    "    r = scan_results_df(str(d))\n",
    "    scan_dfs[scan_id] = r.scanners\n",
    "\n",
    "meta_df = pd.DataFrame(scan_meta).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Combine all scanner results into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "for scan_id, scanners in scan_dfs.items():\n",
    "    for scanner_name, df in scanners.items():\n",
    "        all_rows.append(df)\n",
    "\n",
    "combined = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "# Cast answer to float for numeric scanners\n",
    "combined[\"answer_float\"] = pd.to_numeric(combined[\"answer\"], errors=\"coerce\")\n",
    "# validation_result as bool\n",
    "combined[\"valid\"] = combined[\"validation_result\"] == \"true\"\n",
    "\n",
    "print(f\"Total rows: {len(combined)}\")\n",
    "print(\"Scanner names:\", combined[\"scanner_name\"].unique().tolist())\n",
    "combined[[\"scan_id\", \"scanner_name\", \"transcript_id\", \"answer_float\",\n",
    "          \"validation_target\", \"validation_result\"]].head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation overview per scan × scanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only rows that have a validation target\n",
    "val = combined.dropna(subset=[\"validation_target\"]).copy()\n",
    "val[\"validation_target\"] = pd.to_numeric(val[\"validation_target\"], errors=\"coerce\")\n",
    "\n",
    "summary = (\n",
    "    val.groupby([\"scan_id\", \"scanner_name\"])\n",
    "    .agg(\n",
    "        n_validated=(\"valid\", \"count\"),\n",
    "        n_correct=(\"valid\", \"sum\"),\n",
    "    )\n",
    "    .assign(accuracy=lambda x: x[\"n_correct\"] / x[\"n_validated\"])\n",
    "    .reset_index()\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scanner answer vs. validation target — scatter / strip plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on answer_format scanner which has numeric targets\n",
    "af = val[val[\"scanner_name\"] == \"answer_format\"].copy()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(af[\"scan_id\"].unique()), figsize=(5 * len(af[\"scan_id\"].unique()), 4), sharey=True)\n",
    "if not hasattr(axes, \"__iter__\"):\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (sid, grp) in zip(axes, af.groupby(\"scan_id\")):\n",
    "    jitter = np.random.default_rng(0).uniform(-0.15, 0.15, len(grp))\n",
    "    colors = grp[\"valid\"].map({True: \"steelblue\", False: \"tomato\"})\n",
    "    ax.scatter(grp[\"validation_target\"] + jitter, grp[\"answer_float\"], c=colors, alpha=0.7, edgecolors=\"k\", linewidths=0.4)\n",
    "    # Perfect-agreement diagonal\n",
    "    lim = [-0.5, 3.5]\n",
    "    ax.plot(lim, lim, \"k--\", lw=0.8, alpha=0.5, label=\"perfect\")\n",
    "    ax.set_xlim(lim); ax.set_ylim(lim)\n",
    "    ax.set_xlabel(\"Validation target\"); ax.set_ylabel(\"Scanner answer\")\n",
    "    ax.set_title(f\"scan_id=...{sid[-6:]}\")\n",
    "    from matplotlib.patches import Patch\n",
    "    ax.legend(handles=[Patch(color=\"steelblue\", label=\"correct\"), Patch(color=\"tomato\", label=\"wrong\")], fontsize=8)\n",
    "\n",
    "fig.suptitle(\"answer_format: scanner prediction vs. validation target\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion matrix (answer_format, latest scan with validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the scan with the most validation entries\nbest_scan = summary[summary[\"scanner_name\"] == \"answer_format\"].sort_values(\"n_validated\", ascending=False).iloc[0][\"scan_id\"]\ngrp = af[af[\"scan_id\"] == best_scan].dropna(subset=[\"answer_float\", \"validation_target\"])\n\ny_true = grp[\"validation_target\"].astype(int)\ny_pred = grp[\"answer_float\"].round().astype(int)\n\ncm = pd.crosstab(y_true, y_pred, rownames=[\"True\"], colnames=[\"Predicted\"])\n\nfig, ax = plt.subplots(figsize=(5, 4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\nax.set_title(f\"Confusion matrix — answer_format\\n(scan ...{best_scan[-6:]})\")\nplt.tight_layout()\nplt.show()\n\n# Per-class precision/recall computed manually\nlabels = sorted(set(y_true) | set(y_pred))\nrows = []\nfor lbl in labels:\n    tp = ((y_true == lbl) & (y_pred == lbl)).sum()\n    fp = ((y_true != lbl) & (y_pred == lbl)).sum()\n    fn = ((y_true == lbl) & (y_pred != lbl)).sum()\n    prec = tp / (tp + fp) if (tp + fp) else float(\"nan\")\n    rec  = tp / (tp + fn) if (tp + fn) else float(\"nan\")\n    f1   = 2 * prec * rec / (prec + rec) if (prec + rec) else float(\"nan\")\n    rows.append({\"label\": lbl, \"support\": int((y_true == lbl).sum()),\n                 \"precision\": round(prec, 3), \"recall\": round(rec, 3), \"f1\": round(f1, 3)})\nreport = pd.DataFrame(rows).set_index(\"label\")\nprint(report)\nprint(f\"\\nOverall accuracy: {(y_true == y_pred).mean():.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Validation accuracy vs. transcript outcome\n",
    "\n",
    "Do scanner errors cluster on transcripts where the agent succeeded (`C`) or failed (`I`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_best = af[af[\"scan_id\"] == best_scan].copy()\n",
    "\n",
    "cross = (\n",
    "    af_best.groupby([\"transcript_score\", \"valid\"])\n",
    "    .size()\n",
    "    .unstack(\"valid\", fill_value=0)\n",
    "    .rename(columns={True: \"scanner correct\", False: \"scanner wrong\"})\n",
    ")\n",
    "cross[\"accuracy\"] = cross[\"scanner correct\"] / (cross[\"scanner correct\"] + cross[\"scanner wrong\"])\n",
    "print(cross)\n",
    "\n",
    "cross[[\"scanner correct\", \"scanner wrong\"]].plot(\n",
    "    kind=\"bar\", stacked=True, color=[\"steelblue\", \"tomato\"],\n",
    "    figsize=(5, 3), title=\"Scanner validation accuracy by agent outcome\",\n",
    "    xlabel=\"Agent transcript score (C=correct, I=incorrect)\",\n",
    "    ylabel=\"Count\"\n",
    ")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-scan comparison — accuracy over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_map = meta_df.set_index(\"scan_id\")[\"timestamp\"]\n",
    "plot_data = (\n",
    "    summary[summary[\"scanner_name\"] == \"answer_format\"]\n",
    "    .assign(timestamp=lambda x: x[\"scan_id\"].map(ts_map))\n",
    "    .sort_values(\"timestamp\")\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "ax.bar(range(len(plot_data)), plot_data[\"accuracy\"], color=\"steelblue\")\n",
    "ax.set_xticks(range(len(plot_data)))\n",
    "ax.set_xticklabels([f\"...{sid[-6:]}\\n{ts[:10]}\" for sid, ts in zip(plot_data[\"scan_id\"], plot_data[\"timestamp\"])], fontsize=8)\n",
    "ax.axhline(1.0, color=\"gray\", linestyle=\"--\", lw=0.8)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_ylabel(\"Validation accuracy\")\n",
    "ax.set_title(\"answer_format validation accuracy across scan runs\")\n",
    "for i, (acc, n) in enumerate(zip(plot_data[\"accuracy\"], plot_data[\"n_validated\"])):\n",
    "    ax.text(i, acc + 0.02, f\"{acc:.2f}\\n(n={n})\", ha=\"center\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error analysis — which transcripts are consistently mis-predicted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All answer_format rows with validation\n",
    "error_rows = af[~af[\"valid\"]].copy()\n",
    "error_rows[\"error\"] = error_rows[\"answer_float\"].round().astype(\"Int64\").astype(str) + \" → \" + error_rows[\"validation_target\"].astype(\"Int64\").astype(str)\n",
    "\n",
    "# Count how many scans each transcript fails in\n",
    "fail_counts = (\n",
    "    error_rows.groupby(\"transcript_id\")\n",
    "    .agg(\n",
    "        fail_in_n_scans=(\"scan_id\", \"nunique\"),\n",
    "        errors=(\"error\", lambda x: \", \".join(sorted(set(x)))),\n",
    "        transcript_score=(\"transcript_score\", \"first\"),\n",
    "        validation_target=(\"validation_target\", \"first\"),\n",
    "    )\n",
    "    .sort_values(\"fail_in_n_scans\", ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "fail_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}