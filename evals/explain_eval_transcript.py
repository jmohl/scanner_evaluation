#!/usr/bin/env python3
"""
explain_eval_transcript.py

Explains the structure and fields of .eval transcript files generated by inspect_ai.

Usage:
    python evals/explain_eval_transcript.py [path/to/file.eval]

If no path is provided, it searches for the first .eval file in
evals/core_bench/eval-logs/.
"""

import json
import sys
import zipfile
from pathlib import Path


SEPARATOR = "=" * 70


def section(title: str) -> None:
    print(f"\n{SEPARATOR}")
    print(f"  {title}")
    print(SEPARATOR)


def field(name: str, value, description: str) -> None:
    print(f"\n  [{name}]")
    print(f"    Description : {description}")
    print(f"    Example     : {json.dumps(value, default=str)[:200]}")


def explain_header(data: dict) -> None:
    section("FILE: header.json  —  Top-level eval metadata")
    print("""
  This is the primary metadata file for the entire eval run.
  It is written once the run completes (unlike _journal/start.json
  which is written at the start).
""")

    ev = data.get("eval", {})
    res = data.get("results", {})
    stats = data.get("stats", {})
    plan = data.get("plan", {})

    field("version", data.get("version"),
          "File format version (integer, currently 2).")
    field("status", data.get("status"),
          "'success', 'error', or 'cancelled'.")
    field("invalidated", data.get("invalidated"),
          "True if this log has been marked invalid after the fact.")

    print("\n  -- eval block --")
    field("eval.eval_id", ev.get("eval_id"),
          "Unique ID for this evaluation task definition.")
    field("eval.run_id", ev.get("run_id"),
          "Unique ID for this specific run (changes on each re-run).")
    field("eval.created", ev.get("created"),
          "ISO-8601 timestamp when the run was created.")
    field("eval.task", ev.get("task"),
          "The inspect_ai task identifier, e.g. 'inspect_evals/core_bench'.")
    field("eval.task_id", ev.get("task_id"),
          "Short hash ID derived from task args (stable across identical runs).")
    field("eval.task_version", ev.get("task_version"),
          "Semantic version of the task definition.")
    field("eval.task_display_name", ev.get("task_display_name"),
          "Human-readable task name.")
    field("eval.task_args", ev.get("task_args"),
          "All task arguments (including defaults) used for this run.")
    field("eval.task_args_passed", ev.get("task_args_passed"),
          "Only the task arguments explicitly passed by the caller (subset of task_args).")
    field("eval.dataset", ev.get("dataset"),
          "Dataset metadata: name, location, total samples, and sample IDs evaluated.")
    field("eval.model", ev.get("model"),
          "Model identifier used, e.g. 'openai/gpt-5-mini-2025-08-07'.")
    field("eval.model_generate_config", ev.get("model_generate_config"),
          "Extra generation parameters passed to the model (temperature, etc.).")
    field("eval.model_args", ev.get("model_args"),
          "Extra arguments for the model provider client.")
    field("eval.config", ev.get("config"),
          ("Run-level config: epochs, epochs_reducer, fail_on_error, "
           "sandbox_cleanup, log_samples, log_realtime, log_images, score_display."))
    field("eval.revision", ev.get("revision"),
          "Git revision info (origin, commit hash, dirty flag) for reproducibility.")
    field("eval.packages", ev.get("packages"),
          "Versions of inspect_ai and inspect_evals used.")
    field("eval.scorers", ev.get("scorers"),
          "Scorer definitions: name, options, and metrics computed.")

    print("\n  -- plan block --")
    field("plan.name", plan.get("name"),
          "Name of the solver plan (usually 'plan').")
    field("plan.steps", plan.get("steps"),
          ("Ordered list of solver steps executed. Each step has: "
           "'solver' (step type), 'params' (all params), "
           "'params_passed' (explicitly passed params)."))
    field("plan.config", plan.get("config"),
          "Additional plan-level config (usually empty).")

    print("\n  -- results block --")
    field("results.total_samples", res.get("total_samples"),
          "Total number of samples in the dataset slice evaluated.")
    field("results.completed_samples", res.get("completed_samples"),
          "Number of samples that completed without error.")
    field("results.scores", res.get("scores"),
          ("List of scorer results. Each entry has: scorer name, "
           "scored_samples, unscored_samples, params, and metrics "
           "(e.g. 'accuracy': {'name', 'value', 'params'})."))

    print("\n  -- stats block --")
    field("stats.started_at", stats.get("started_at"),
          "ISO-8601 timestamp when the run started.")
    field("stats.completed_at", stats.get("completed_at"),
          "ISO-8601 timestamp when the run finished.")
    field("stats.model_usage", stats.get("model_usage"),
          ("Per-model token usage totals: input_tokens, output_tokens, "
           "total_tokens, input_tokens_cache_read, reasoning_tokens."))


def explain_journal_start(data: dict) -> None:
    section("FILE: _journal/start.json  —  Run start snapshot")
    print("""
  Written at the beginning of the run before any samples are processed.
  Contains the same 'eval' and 'plan' blocks as header.json but without
  'results' or 'stats' (those are only available after completion).
  Useful for inspecting what the run was configured to do if it crashed.
""")


def explain_summaries_json(data: list) -> None:
    section("FILE: summaries.json  —  Per-sample summary records")
    print("""
  A JSON array with one entry per (sample, epoch) pair.
  This is the primary file for aggregating results across samples.
""")
    if not data:
        return
    s = data[0]
    field("id", s.get("id"),
          "Sample identifier, e.g. 'capsule-6049678'.")
    field("epoch", s.get("epoch"),
          "Epoch number (1-based). Epochs > 1 appear when eval.config.epochs > 1.")
    field("input", s.get("input"),
          "The user-facing prompt / task description shown to the model.")
    field("target", s.get("target"),
          "The expected answer (may be empty if scoring is done programmatically).")
    field("metadata", s.get("metadata"),
          ("Task-specific metadata. For core_bench: field, language, capsule_id, "
           "and 'results' (list of acceptable answer dicts for the scorer)."))
    field("scores", s.get("scores"),
          ("Dict keyed by scorer name. Each entry has: "
           "'value' (model's answer label, e.g. 'C'/'I'), "
           "'answer' (correct label), "
           "'explanation' (JSON string with correct/total counts), "
           "'history' (list of prior score attempts)."))
    field("model_usage", s.get("model_usage"),
          "Token usage for this sample broken down by model.")
    field("started_at", s.get("started_at"),
          "ISO-8601 timestamp when this sample started processing.")
    field("completed_at", s.get("completed_at"),
          "ISO-8601 timestamp when this sample finished.")
    field("total_time", s.get("total_time"),
          "Wall-clock seconds from start to completion (includes waiting).")
    field("working_time", s.get("working_time"),
          "Seconds the model was actively generating (excludes sandbox overhead).")
    field("uuid", s.get("uuid"),
          "Internal unique ID for this sample run instance.")
    field("retries", s.get("retries"),
          "Number of times this sample was retried due to transient errors.")
    field("completed", s.get("completed"),
          "True if the sample finished without error.")
    field("message_count", s.get("message_count"),
          "Total number of messages in the conversation for this sample.")


def explain_reductions_json(data: list) -> None:
    section("FILE: reductions.json  —  Per-scorer per-sample reduced scores")
    print("""
  Contains the final reduced score for each (scorer, sample) pair after
  applying the epochs_reducer (e.g. 'mean' across epochs).
  Useful for building per-sample score tables.
""")
    if not data or not data[0].get("samples"):
        return
    r = data[0]
    s = r["samples"][0]
    field("scorer", r.get("scorer"),
          "Name of the scorer these samples belong to.")
    field("samples[].value", s.get("value"),
          "Reduced numeric score for this sample (e.g. 1.0 = correct, 0 = wrong).")
    field("samples[].answer", s.get("answer"),
          "The correct answer label (e.g. 'C' for Correct).")
    field("samples[].explanation", s.get("explanation"),
          ("JSON string with breakdown of correct/total question counts "
           "split by type (written vs vision)."))
    field("samples[].history", s.get("history"),
          "List of previous score values across epochs (before reduction).")
    field("samples[].sample_id", s.get("sample_id"),
          "The sample ID this reduction corresponds to.")


def explain_sample_file(data: dict) -> None:
    section("FILE: samples/{capsule_id}_epoch_{n}.json  —  Full conversation transcript")
    print("""
  One file per (sample, epoch). Contains the complete message-by-message
  conversation between the system prompt, the model, and tool calls.
  This is the most detailed record of what the model actually did.
""")
    field("id", data.get("id"),
          "Sample identifier matching summaries.json.")
    field("epoch", data.get("epoch"),
          "Epoch number for this transcript.")
    field("input", data.get("input"),
          "The task prompt (same as summaries.json).")
    field("target", data.get("target"),
          "Expected answer (same as summaries.json).")
    field("sandbox", data.get("sandbox"),
          "Sandbox configuration used: type (e.g. 'docker') and compose config path.")
    field("files", data.get("files"),
          "Files copied into the sandbox at the start (e.g. the capsule tarball).")
    field("setup", data.get("setup"),
          "Shell command run inside the sandbox to initialize the environment.")
    field("output", data.get("output"),
          ("The model's final generation output. Contains: model, choices "
           "(last assistant message + stop_reason), completion (text), "
           "usage (token counts), and time (generation latency in seconds)."))

    print("\n  -- messages array --")
    print("""
  messages is an ordered list of all conversation turns. Each message has:
""")

    msgs = data.get("messages", [])
    # Show one of each role
    shown_roles = set()
    for msg in msgs:
        role = msg.get("role")
        if role in shown_roles:
            continue
        shown_roles.add(role)

        print(f"  Role: '{role}'")
        field("  messages[].id", msg.get("id"),
              "Unique message ID.")
        field("  messages[].role", msg.get("role"),
              "'system', 'user', 'assistant', or 'tool'.")
        field("  messages[].source", msg.get("source"),
              ("How the message was produced: 'input' (from the dataset), "
               "'generate' (model output), or absent for tool responses."))

        content = msg.get("content")
        if isinstance(content, str):
            field("  messages[].content (string)", content[:80],
                  "Plain text content (system prompts and tool results use this form).")
        elif isinstance(content, list):
            print(f"\n  messages[].content (list) — content blocks:")
            for block in content:
                btype = block.get("type", "unknown")
                if btype == "text":
                    field(f"    content block: text", block.get("text", "")[:80],
                          "Plain text from the model.")
                elif btype == "reasoning":
                    field(f"    content block: reasoning", "(redacted)" if block.get("redacted") else block.get("summary", "")[:80],
                          ("Extended thinking / reasoning block. Contains: "
                           "'reasoning' (encrypted token if redacted), "
                           "'summary' (human-readable summary), "
                           "'signature' (integrity token), "
                           "'redacted' (bool)."))

        if "tool_calls" in msg:
            tc = msg["tool_calls"][0] if msg["tool_calls"] else {}
            field("  messages[].tool_calls[].id", tc.get("id"),
                  "Unique ID correlating the call to its tool result message.")
            field("  messages[].tool_calls[].function", tc.get("function"),
                  "Name of the tool called (e.g. 'bash', 'text_editor', 'submit').")
            field("  messages[].tool_calls[].arguments", tc.get("arguments"),
                  "Dict of arguments passed to the tool.")
            field("  messages[].tool_calls[].type", tc.get("type"),
                  "Always 'function' for function-calling tools.")
            if "view" in tc:
                field("  messages[].tool_calls[].view", tc.get("view"),
                      "Display hint for the inspect viewer: title, format, content.")

        if msg.get("role") == "assistant":
            field("  messages[].model", msg.get("model"),
                  "The model that generated this assistant turn.")

        if msg.get("role") == "tool":
            field("  messages[].tool_call_id", msg.get("tool_call_id"),
                  "ID of the tool_call this result is responding to.")
            field("  messages[].function", msg.get("function"),
                  "Name of the function that was called.")

        print()


def explain_journal_summary(data: dict) -> None:
    section("FILE: _journal/summaries/{n}.json  —  Incremental sample summary")
    print("""
  Written after each sample completes during the run (for live progress).
  Same structure as one entry in summaries.json.
  Allows partial results to be read from an in-progress eval log.
""")


def main() -> None:
    # Find the .eval file
    if len(sys.argv) > 1:
        eval_path = Path(sys.argv[1])
    else:
        log_dir = Path("evals/core_bench/eval-logs")
        eval_files = list(log_dir.glob("*.eval"))
        if not eval_files:
            print("No .eval files found in evals/core_bench/eval-logs/")
            sys.exit(1)
        eval_path = eval_files[0]

    print(f"\nAnalyzing: {eval_path}")
    print(f"\nA .eval file is a ZIP archive containing multiple JSON files.")
    print(f"Each file captures a different aspect of the eval run.")

    with zipfile.ZipFile(eval_path) as zf:
        names = zf.namelist()
        print(f"\nFiles inside this archive ({len(names)} total):")
        for name in names:
            print(f"  {name}")

        # header.json
        header = json.loads(zf.read("header.json"))
        explain_header(header)

        # _journal/start.json
        start = json.loads(zf.read("_journal/start.json"))
        explain_journal_start(start)

        # summaries.json
        summaries = json.loads(zf.read("summaries.json"))
        explain_summaries_json(summaries)

        # reductions.json
        reductions = json.loads(zf.read("reductions.json"))
        explain_reductions_json(reductions)

        # One sample file
        sample_files = [n for n in names if n.startswith("samples/")]
        if sample_files:
            sample = json.loads(zf.read(sample_files[0]))
            explain_sample_file(sample)

        # _journal/summaries
        journal_summary_files = [n for n in names if n.startswith("_journal/summaries/")]
        if journal_summary_files:
            j = json.loads(zf.read(journal_summary_files[0]))
            explain_journal_summary(j)

    section("SCORE VALUES REFERENCE")
    print("""
  Score values used by the evaluate_task_questions scorer:

  value / answer field:
    'C'  = Correct  (model answered correctly)
    'I'  = Incorrect (model answered incorrectly)
    'P'  = Partial

  reductions[].samples[].value (numeric):
    1.0  = Fully correct
    0.0  = Fully incorrect
    0.0–1.0 = Partial (fraction of questions answered correctly)

  explanation field (JSON string):
    {
      "correct_written_answers": <int>,   # text/numeric questions correct
      "correct_vision_answers":  <int>,   # figure/image questions correct
      "total_written_questions": <int>,
      "total_vision_questions":  <int>
    }
""")

    section("QUICK REFERENCE: File Purposes")
    print("""
  header.json              → Full run metadata + aggregate results + stats
  _journal/start.json      → Run config snapshot at start (no results yet)
  _journal/summaries/{n}   → Live per-sample progress written during the run
  summaries.json           → All per-sample summaries (complete, after run)
  reductions.json          → Per-sample numeric scores (after epoch reduction)
  samples/{id}_epoch_{n}   → Full message transcript for one (sample, epoch)
""")


if __name__ == "__main__":
    main()
